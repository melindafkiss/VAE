\documentclass[12pt]{amsart}
\usepackage{amsmath,amsthm,amssymb,verbatim}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[magyar]{babel}
\selectlanguage{magyar}
%\usepackage{graphicx}
%
\pagestyle{headings}
%
%\input epsf
%
\setlength{\textwidth}{6.5true in}
\setlength{\oddsidemargin}{0 truept}
\setlength{\evensidemargin}{0 truept}
\setlength{\textheight}{8.9 true in}
\parskip=0pt plus 2pt
%
\input latexmacros
\thispagestyle{empty}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\kl}{KL}
%
\begin{document}


\begin{center}
\textbf{Variational autoencoder}
\end{center}

\vspace{20pt}

Jelölések?

Through this tutorial we will use the MNIST dataset to
demonstrate how the model works.

\noindent
\textbf{Generative model}

\vspace{10pt}

The purpose of a generative model, is to estimate the
probability density function (PDF) of the training data. The
VAE model can also sample examples from the learned PDF, so
it'll be able to generate new examples that are similar to
the original dataset. The PDF isn't always learnable, so we
would like to model a distribution, which approximates the
unknown distribution.

\vspace{5pt}

For example if such a model is trained on the MNIST dataset,
it should assign high probability to an image of a digit,
and low probability to a picture an image of random
gibberish. 

\vspace{5pt}

These models have many applications, e.g. generating fake
human faces, or purely synthetic music. One of the most
popular generative models is the variational autoencoder. 

\vspace{10pt}

\noindent
\textbf{Latent variable model}

\vspace{10pt}

If the pixels were independent of each other, we could learn
the PDF of every pixel independently, which is easy, so is
the sampling part, we can sample the pixels independently. 

\vspace{5pt}

In digit images there are clear dependencies between the
pixels. For example if we see the left half of a four in an
image, then we can't see the right part of a zero, because
that's clearly not a digit. 

\vspace{5pt}

These dependencies are coded in tha latent space. We can
think of it as a $k$-dimensional vectorspace $\mathbb{R}^k$,
where every vector contains $k$ essential information about
how to draw a number. For example the first digit could be
the number represented by the digit, the second the width,
etc.

\vspace{10pt}

\noindent
\textbf{Problem scenario}

\vspace{10pt}

We have a dataset,
$\mathcal{D} =\{ x^{(1)}, x^{(1)}, \dots, x^{(n)}\}$, which
we assume is i.i.d, from an unknown $p^*$ distribution. This
is the training set. Also, we assume that our data point $x$
is generated by the following two-step process:
\begin{itemize}
\item $z^{(i)}$ is sampled from a simple prior distribution
  $p(z)$,
\item $x^{(i)}$ is sampled from a complex but tractable
  conditional distribution $p(x|z)$.
\end{itemize}

So first the process decides what properties the digit will have,
and then it draws the digit. 

\vspace{10pt}

\noindent
\textbf{Intractabilities}

\vspace{10pt}

By definition, $p(x) = E_{z \sim p(z)} p(x|z)$, but in
practice this integral is too complex for us to handle for
more interesting models. So tipically when doing maximum
likelihood (ML) estimation, we cannot optimize the marginal
with respect to its parameters.

\vspace{5pt}

We could use Monte Carlo methods, namely take $N$ samples
from the prior distribution: $\{z_1, \dots, z_N\}$ and
approximate the marginal by
\[
  p(x)\approx \frac{1}{N} \sum_{i=1}^N p(x|z_i).
\]

\vspace{5pt}

Unfortunately the dimension of the $x$ random variable is
huge, so $N$ needs to be a very large number to achieve a
relatively good approximation. So we would like to find a
method which works in the case of intractability, and when
sampling based methods are too slow.

\vspace{10pt}

\textbf{The prior and the posterior}

\vspace{10pt}

Question is, how to define the latent variables. The
decisions the model has to make are very complicated. It has
to decide which digit to draw, what style it will have,
etc. Also, these attributes can be correlated, e.g. if
someone writes very fast, the number is usually slanted and
slender at the same time.

\vspace{5pt}

Ideally we don't want to set these properties ourselves,
also we would like to avoid describing the dependencies
between the coordinates. The VAE does neither, instead we
can get an element of the latent space by sampling from the
standard normal distribution, so we set
$p(z) = \mathcal{N}(0, I)$. This works, because we can
obtain any distribution in $d$ dimensions by mapping the
standard normal distribution in $d$ dimensions through a
sufficiently complicated function.

\vspace{5pt}

So what happens is that the first few layers of the model
learns the function which maps the normally distributed $z$
values to whatever latent values are needed for the model,
and then map those to a digit $x$.

\vspace{5pt}

We assume that the posterior is from a family of
distributions with some $\theta$ unknown parameter. Our goal
is to learn this parameter vector so the above process
maximizes the likelihood of the dataset
$\mathcal{D}$. Instead of the marginal distribution, we will
work with its logarithm, as the maximum point is the
same. As the training set is i.i.d, we want to maximize the
following sum: $\sum_{i=1}^n \log p_{\theta}(x^{(i)})$. So
we can deal with the elements of the training set
separately. We'll now add a subscript $\theta$ to make this
explicit, and write $p_\theta(x|z)$ from now on. With
formula:
\[
  \argmax_{\theta} \sum_{i=1}^n \log E_{z \sim p(z)}
  p_{\theta}(x^{(i)}|z).
\]

We will denote the empirical data distribution by $\hat{P}$,
which is simply the discrete uniform distibution over our
dataset. The above formula using the $\hat{P}$ notation:

\[
\argmax_{\theta} E_{x \sim \hat{P}} \log E_{z \sim p(z)}
P_{\theta}(x|z).
\]

This can also be formulated as minimizing the KL-divergence
between the $\hat{P}$ empirical data distribution and the
$p(x)$ marginal distribution:

\[
  \kl(\hat{P} \| p_{\theta}(x)) = E_{x \sim \hat{P}} [\log
  \hat{P}(x)] - E_{x \sim \hat{P}} [\log p_{\theta}(x)]
\]

which is a constant minus the log-likelihood.

In our case we choose the $p_\theta(x| z)$ posterior to be a
normal distribution $\mathcal{N}(f(z, \theta), \sigma^2 I)$,
where $\sigma$ is a hyperparameter. The output of the model
is $f(z, \theta)$.

\vspace{5pt}

Even though the prior and the posterior are relatively
simple, the marginal can be very complex, so we can use it
to approximate the real underlying distribution.

\vspace{10pt}

\textbf{Shortcut}

\vspace{10pt}

Instead of the marginal distribution, we will work with its
logarithm, as the maximum point is the same. As the training
set is i.i.d, we want to maximize the following sum:
$\sum_{i=1}^n \log p_{\theta}(x^{(i)})$. So we can deal with
the elements of the training set separately. 

\vspace{5pt}

We said earlier, that we could approximate the marginal
distribution by sampling, but we would need many
samples. The question is whether we can make it faster. In
practice, for most $z$, the value $p_\theta(x^{(i)}|z)$ is
very small, so it contributes almost nothing to our
estimate. 

\vspace{5pt}

The key idea behind the Variational Autoencoder is that we
want to sample values of $z$ that are likely to have
produced $x^{(i)}$ and compute $p(x^{(i)})$ just from
those. This is described exactly by the likelihood,
$p(z|x^{(i)})$.

\vspace{5pt}

Unfortunately it is intractable, so we need another
tractable distribution on the latent space, $q(z|x^{(i)})$,
which approximates the likelihood. We hope that the set of
such $z$-s, for which the this value is large is smaller,
than thoses $z$-s, which are very likely according to the
prior. If this is true, then e.g. we can compute $E_{z\sim
  q}p(x^{(i)}|z)$ relatively easily, we will see why this is
important. 

\vspace{5pt}

A question arises immediately: if we get a $z$ value by
sampling from a distribution, whose PDF is this $q$ and not
the standard normal distribution, why does that help in
optimizing the marginal distribution. 

\vspace{5pt}

We would like to note, that now we can see that we got the
standard autoencoder structure, the $q$ models the encoder
and the posterior models the decoder.

\vspace{10pt}

\textbf{Reforming the Kullback-Leibler divergence}

\vspace{10pt}

What we will do is relate the marginal distribution with the
$E_{z\sim q}p(x^{(i)}|z)$ expected value, we will see where
$q$ comes from later. First, we examine the KL-divergence
between $q(z|x^{(i)})$ and $p_\theta(z| x^{(i)})$. By
definition 
\[
  D_{KL}(q(z| x^{(i)}) \| p_\theta(z| x^{(i)})) =
  E_{q(z|x^{(i)})}(\log q(z|x^{(i)})-\log
  p_{\theta}(z| x^{(i)})).
\]

\vspace{5pt}

By Bayes' theorem
\[
  p_{\theta}(z| x^{(i)}) = \frac{p_{\theta}(x^{(i)}\mid
    z)p(z)}{p_{\theta}(x^{(i)})}.
\]

\vspace{5pt}

So taking the logarithm of both sides: 
\[
  \log p_{\theta}(z| x^{(i)}) = \log p_{\theta}(x^{(i)}\mid
  z) + \log p(z) - \log p_{\theta}(x^{(i)}).
\]

\vspace{5pt}

Writing this in we get the following: 

\[
  D_{KL}(q(z| x^{(i)}) \| p_\theta(z| x^{(i)})) =
  E_{q(z|x^{(i)})}\Big(\log q(z|x^{(i)})- \log
  p_{\theta}(x^{(i)}| z)-\log p(z)+ \log
  p_{\theta}(x^{(i)})\Big).
\]

\vspace{5pt}

As the last term in the expectation doesn't depend on $z$,
we can take it out: 

\[
  D_{KL}(q(z| x^{(i)}) \| p_\theta(z| x^{(i)})) =
  E_{q(z|x^{(i)})}\Big(\log q(z|x^{(i)})- \log
  p_{\theta}(x^{(i)}| z)-\log p(z) \Big) +\log
  p_{\theta}(x^{(i)}).
\]

After rearranging 

\[
  \log p_{\theta}(x^{(i)}) - D_{KL}(q(z| x^{(i)}) \|
  p_\theta(z| x^{(i)})) = E_{q(z|x^{(i)})}\Big( \log
  p_{\theta}(x^{(i)}| z)+ \log p(z)- \log
  q(z|x^{(i)}) \Big).
\]

\vspace{5pt}

We can rearrange the right hand side, as

\[
  D_{KL}(q_{\phi}(z| x^{(i)})\| p(z)) =
  E_{q(z|x^{(i)})}\Big( \log q(z|x^{(i)}) - p(z) \Big),
\]

\vspace{5pt}

so 
\[
  \log p_{\theta}(x^{(i)}) - D_{KL}(q(z| x^{(i)}) \|
  p_\theta(z| x^{(i)})) = E_{q(z|x^{(i)})}\Big( \log
  p_{\theta}(x^{(i)}| z)\Big) - D_{KL}(q_{\phi}(z|
  x^{(i)})\| p(z)). 
\]

\vspace{5pt}

This last equation is the esssence of the variational
autoencoder. On the left hand size we have the quantity we
want to maximize. Also, we have another term, a
Kullback-Leibler divergence which we want to minimize, as we
want the $q$ distribution to approximate the likelihood. So
by maximizing the left hand side we kill two birds with one
stone. 

\vspace{10pt}

\textbf{ELBO}

\vspace{10pt}

The main difference between the VAE and the standard
autoencoder lies in the latent space. The main advantage of
the VAE is that it learns a smooth latent representation of
the input, while the standard autoencoder only learns a
function with whose help we can reconstruct the input.

\vspace{5pt}

We can see on the left hand side what happens if we focus
only on the reconstruction loss. We can separate the
distinct classes from each other, so we can recontruction is
easier. Unfortunately there are areas in the latent space,
which doesn't represent any observed datapoint.

\vspace{5pt}

On the other hand if we focus only in the Kullback-Leibler
divergence, (so what we want is to pull the latent
distribution to the prior), then we describe every datapoint
with the same distriubtion, namely the standard normal. In
this case the model thinks that every observation has the
same characteristics, so we cant't characterize the original
dataset. 

\vspace{5pt}

But if we optimize the two together, then we get a latent
space which maintains the similarity of nearby encodings, so
we get clusters again, but globally it is densely packed
near the latent space origin.

\vspace{5pt}

Now let's look at the right hand side. It is usually called
ELBO and it is denoted by
\[
    \mathcal{L}(\theta, \phi, x^{(i)})= E_{q_{\phi}(z|
      x^{(i)})}\log p_{\theta}(x^{(i)}| z) -
    D_{KL}(q_{\phi}(z| x^{(i)})\| p(z)).
\]

\vspace{5pt}

It is also called Variational Lower Bound. It is indeed a
lower bound, as the Kullback-Leibler divergence is always
non-negative. So instead of maximizing the left hand side,
we maximize the ELBO, which is more managable, as we will
see. 

\vspace{5pt}

Usually the $q$ distribution is chosen to be a normal
distribution, namely
$q(z| x^{(i)}) = \mathcal{N}(\mu_\phi(x^{(i)}),
\Sigma_\phi(x^{(i)}))$. It is parametrized by a neural
network with parameters $\phi$, the $\mu$ and $\Sigma$ are
deterministic functions depending on $\phi$, the are learned
by the network. Usually they chose the $\Sigma$ to be
diagonal.

\vspace{5pt}

The reason for this is that we can compute the
Kullback-Leibler divergence between two normal distributions
in closed form. Let $\mathcal{N}(\mu_0, \Sigma_0)$ and
$\mathcal{N}(\mu_1, \Sigma_1)$ be arbitrary normal
distributions. Then their KL-divergence is: x
\[
    D_{KL}(\mathcal{N}(\mu_0, \Sigma_0)\| \mathcal{N}(\mu_1,
    \Sigma_1)) =
\]
\[
    = \frac{1}{2}\Big(\tr (\Sigma_1^{-1}\Sigma_0)+
    (\mu_1-\mu_0)^T \Sigma_1^{-1}(\mu_1-\mu_0)-k +\log
    \Big(\frac{\det \Sigma_1}{\det \Sigma_0}\Big)\Big),
\]  

\vspace{5pt}

where $k$ is the dimension of the distributions. In our
case: 
\[
  D_{KL}(\mathcal{N}(\mu_\phi(x^{(i)}),
  \Sigma_\phi(x^{(i)}))\| \mathcal{N}(0,I)) =
\]
\[
  = \frac{1}{2}\Big(\tr (\Sigma_\phi(x^{(i)}))+
  (\mu_\phi(x^{(i)})^T (\mu_\phi(x^{(i)})-k +\log
  \det(\Sigma_\phi(x^{(i)})\Big)
\]

\vspace{5pt}

The first term is slightly more complicated. We could
approximate it by sampling, but people observed that it is
enough to take only one sample from the $q$ distribution and
for that concrete $z$ value $\log p_{\theta}(x^{(i)}| z)$
will be a fair approximation of the expected value. 

\vspace{5pt}

So we have the logarithm of the PDF of a normal
distribution. Computing it we have a constant term, which
doesn't count, and the squared distance of the $x^{(i)}$ and
$f(z, \theta)$ vectors, which is the reconstruction loss. 

\vspace{5pt}

We would like to find the $\theta$ and$\phi$ parameters, so
we want to take the gradient of the ELBO with respect to
these. The second term doesn't depend on $\theta$, so 
\[
  \nabla_\theta \mathcal{L}(\theta, \phi, x^{(i)}) =
  \nabla_\theta E_{q_{\phi}(z| x^{(i)})}\log
  p_{\theta}(x^{(i)}| z). 
\]

\vspace{5pt}

As $q_{\phi}(z| x^{(i)})$ doesn't depend on $\theta$ as
well, 
\[
  \nabla_\theta E_{q_{\phi}(z| x^{(i)})}\log
  p_{\theta}(x^{(i)}| z) = E_{q_{\phi}(z|
    x^{(i)})}(\nabla_\theta \log p_{\theta}(x^{(i)}| z))\simeq
\]

\[
  \simeq \nabla_\theta \log p_{\theta}(x^{(i)}| z),
\]

\vspace{5pt}

where the last term is the Monte Carlo estimation of the
previous term, and $z$ is a random sample from the $q$
distribution. 

\vspace{10pt}

\textbf{Reparametrization trick}

\vspace{10pt}

Taking the gradient by the $\phi$ parameters, we will have a
problem with the first term, as we cannot take the gradient
into the integral. The real problem is that the
backpropagation needs to go through a layer which gives us a
$z$ vector from the $q$ distribution by sampling, and this
operation is not continuous.

\vspace{5pt}

The idea is that we reparametrize the $z$ random variable as
a function of $x^{(i)}$, $\phi$, and a new random variable
$\vep$. Generally if $z$ is a random variable and $g$ is a
differentiable and invertible transformation, and $z =
g(\vep, \phi, x)$ where the distribution of the $\vep$
random variable $r(\vep)$ doesn't depend on $x$ or $\phi$,
then  
\[
  E_{q_{\phi}(z| x^{(i)})}(f(z)) = E_{r(\vep)}(f(g(\vep,
  \phi, x))).
\]

\vspace{5pt}

In our case the posterior is
$\mathcal{N}(\mu_\phi(x^{(i)}), \Sigma_\phi(x^{(i)}))$, let
$\vep \sim \mathcal{N}(0, I)$. Then
$z = \mu_\phi(x^{(i)} + \Sigma_\phi^{1/2}(x^{(i)})\cdot
\vep$, which is a differentiable, invertible function, so
backpropation works and we can do the same thing as before,
we can take tha gadient into the expectation.

\vspace{10pt}

\textbf{Testing}

\vspace{10pt}

After we trained the model and we wish to test it, we don't
need the encoder part. The only thing we need to do is
sample from the prior and send is through the decoder. 

\end{document}